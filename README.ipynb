{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning\n",
    "\n",
    "Hereafter We present the information for Cloning driving behavior using Deep Learning. The code will be presented separately in the repository in the file `model.py` and different packages will be used to process training data gathered from Udacity's simulator. Images gathered will be extended to provide better traininig. Images were saved to an S3 bucket to provide access from multiple computers to craft the model separately from the training instance.\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Files\n",
    "\n",
    "## Are all required files submitted?\n",
    "\n",
    "> The submission includes a model.py file, drive.py, model.h5 and a writeup report.\n",
    "\n",
    "The following project includes:\n",
    "\n",
    "  - [**model.py**](model.py)\n",
    "      Contains the script to create and train the model\n",
    "  - [**drive.py**](drive.py) For driving the car in autonomous mode\n",
    "  - [**model.h5**](model.h5) Contains a trained convolution neural network \n",
    "  - [**writeup_report.md**](writeup_report.md) Summary of the results\n",
    "  - **data_processing/** Module where all data processing code was placed\n",
    "    - **data_preprocessing.py** Utilities to merge all csv files into one \n",
    "    - **data_processing.py** Utilities to navigate csv using one index structure so that we can perform the following in a simple manner:\n",
    "        - Shuffle the data \n",
    "        - Apply multiple transforms \n",
    "        - Extend the data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality of code\n",
    "\n",
    "## Is the code functional?\n",
    "\n",
    "> The model provided can be used to successfully operate the simulation.\n",
    "\n",
    "Using the Udacity provided simulator and drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "To extend the data I created a standardized process.\n",
    "\n",
    "  1. Gather new data using Udacity's Simulator and save it to a specific folder, e.g. *new_data*. Inside that folder you'll also have a **IMG** directory and a **driving_log.csv** file.\n",
    "  2. Place the directory inside the specific track folder inside the data directory. \n",
    "  3. If you have a **data/driving_log_compiled.csv** file already, erase it and re run *model.py* to regenerate, the process should gather your new images. All the transformations already in place will be applied to the new images.\n",
    "\n",
    "\n",
    "## Is the code usable and readable?\n",
    "\n",
    "> The code in `model.py` uses a Python generator, if needed, to generate data for training rather than storing the training data in memory. The `model.py` code is clearly organized and comments are included where needed.\n",
    "\n",
    "The code is separated in three files.\n",
    "\n",
    "### `model.py`\n",
    "\n",
    "Contains all the code related to the CNN and the generators involved to train the model, as well as the pipeline for training, validating, and evaluating the model.\n",
    "\n",
    "### `data/data_processing.py`\n",
    "\n",
    "Several transformations for our images are done to extend the dataset, more information on this is found below. All the code pertaining into how these transformations are done is contained in this file.\n",
    "\n",
    "### `data/data_preprocessing.py`\n",
    "\n",
    "Images are separated across multiple folders to allow scalability when adding new data from the simulator. These folders and files are traversed and merged into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Training Strategy\n",
    "\n",
    "## Has an appropriate model architecture been employed for the task?\n",
    "\n",
    "> The neural network uses convolution layers with appropriate filter sizes. Layers exist to introduce nonlinearity into the model. The data is normalized in the model.\n",
    "\n",
    "\n",
    "\n",
    "## Has an attempt been made to reduce overfitting of the model?\n",
    "\n",
    "> Train/validation/test splits have been used, and the model uses dropout layers or other methods to reduce overfitting.\n",
    "\n",
    "\n",
    "\n",
    "## Have the model parameters been tuned appropriately?\n",
    "\n",
    "> Learning rate parameters are chosen with explanation, or an Adam optimizer is used.\n",
    "\n",
    "\n",
    "\n",
    "## Is the training data chosen appropriately?\n",
    "\n",
    "> Training data has been chosen to induce the desired behavior in the simulation (i.e. keeping the car on the track).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture and Training Documentation\n",
    "\n",
    "## Is the solution design documented?\n",
    "\n",
    "> The README thoroughly discusses the approach taken for deriving and designing a model architecture fit for solving the given problem.\n",
    "\n",
    "## Is the model architecture documented?\n",
    "\n",
    "> The README provides sufficient details of the characteristics and qualities of the architecture, such as the type of model used, the number of layers, the size of each layer. Visualizations emphasizing particular qualities of the architecture are encouraged.\n",
    "\n",
    "## Is the creation of the training dataset and training process documented?\n",
    "\n",
    "> The README describes how the model was trained and what the characteristics of the dataset are. Information such as how the dataset was generated and examples of images from the dataset should be included.\n",
    "\n",
    "To assemble the data we use multiple use cases, we need to be able to extend the data easily so we create some scripts that rebuilds the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['track1', 'track2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir('./data')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "\n",
    "## Is the car able to navigate correctly on test data?\n",
    "\n",
    "> No tire may leave the drivable portion of the track surface. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe (if humans were in the vehicle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
